{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e6aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from flair.data import Sentence\n",
    "from flair.models.sequence_tagger_utils.bioes import get_spans_from_bio\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import time\n",
    "from functools import partial\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dea46d4-462c-470a-afb1-1bfadcc32c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_array(string, encoding=\"utf-8\", batch=False):\n",
    "    if batch:\n",
    "        return np.asarray(list(bytes(string, encoding))).reshape(1, -1)\n",
    "    else:\n",
    "        return np.asarray(list(bytes(string, encoding)))\n",
    "\n",
    "\n",
    "def bytes_to_string(byte_list):\n",
    "    return bytes(byte_list.tolist()).decode()\n",
    "\n",
    "\n",
    "class ClientDecoder:\n",
    "    def __init__(self, triton_server_url, model_name, model_version):\n",
    "        self.triton_client = grpcclient.InferenceServerClient(\n",
    "            url=triton_server_url, verbose=False\n",
    "        )\n",
    "\n",
    "        self.model_metadata = self.triton_client.get_model_metadata(\n",
    "            model_name=model_name, model_version=model_version\n",
    "        )\n",
    "\n",
    "        self.model_config = self.triton_client.get_model_config(\n",
    "            model_name=model_name, model_version=model_version\n",
    "        ).config\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def submit(self, sentence_bytes, device=\"cpu\"):\n",
    "        inputs = [\n",
    "            grpcclient.InferInput(\"sentence_bytes\", sentence_bytes.shape, \"INT64\"),\n",
    "        ]\n",
    "\n",
    "        inputs[0].set_data_from_numpy(sentence_bytes)\n",
    "\n",
    "        outputs = [grpcclient.InferRequestedOutput(\"tagged_sentences\")]\n",
    "\n",
    "        response = self.triton_client.infer(self.model_name, inputs, outputs=outputs)\n",
    "\n",
    "        tagged_sentences = torch.tensor(\n",
    "            response.as_numpy(\"tagged_sentences\"), device=DEVICE\n",
    "        )\n",
    "\n",
    "        return eval(bytes(tagged_sentences).decode())\n",
    "\n",
    "\n",
    "class ClientDecoderAsync:\n",
    "    def __init__(self, triton_server_url, model_name, model_version):\n",
    "        self.triton_client = grpcclient.InferenceServerClient(\n",
    "            url=triton_server_url, verbose=False\n",
    "        )\n",
    "\n",
    "        self.model_metadata = self.triton_client.get_model_metadata(\n",
    "            model_name=model_name, model_version=model_version\n",
    "        )\n",
    "\n",
    "        self.model_config = self.triton_client.get_model_config(\n",
    "            model_name=model_name, model_version=model_version\n",
    "        ).config\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def callback(self, user_data, result, error):\n",
    "        if error:\n",
    "            user_data.append(error)\n",
    "        else:\n",
    "            user_data.append(result)\n",
    "\n",
    "    def submit(self, sentence_bytes_list, device=\"cpu\"):\n",
    "        triton_inputs = []\n",
    "        for sentence_bytes in sentence_bytes_list:\n",
    "            triton_inputs.append(\n",
    "                [grpcclient.InferInput(\"sentence_bytes\", sentence_bytes.shape, \"INT64\")]\n",
    "            )\n",
    "            triton_inputs[-1][0].set_data_from_numpy(sentence_bytes)\n",
    "\n",
    "        outputs = [grpcclient.InferRequestedOutput(\"tagged_sentences\")]\n",
    "\n",
    "        async_requests = []\n",
    "\n",
    "        for triton_input in triton_inputs:\n",
    "            self.triton_client.async_infer(\n",
    "                model_name=self.model_name,\n",
    "                inputs=triton_input,\n",
    "                callback=partial(self.callback, async_requests),\n",
    "                outputs=outputs,\n",
    "            )\n",
    "\n",
    "        while len(async_requests) != len(triton_inputs):\n",
    "            time.sleep(0.05)\n",
    "\n",
    "        tagged_sentences = []\n",
    "        for response in async_requests:\n",
    "            tagged_sentence = torch.tensor(\n",
    "                response.as_numpy(\"tagged_sentences\"), device=DEVICE\n",
    "            )\n",
    "            tagged_sentence = eval(bytes(tagged_sentence).decode())\n",
    "            tagged_sentences.append(tagged_sentence)\n",
    "\n",
    "        return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f076749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRITON_SERVER_URL = \"172.25.4.42:8001\"\n",
    "MODEL_NAME = \"flair-ner-english-fast-ensemble\"\n",
    "MODEL_VERSION = \"1\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "MULTIPLIER = 128\n",
    "SAMPLE_TEXTS = open(\"strings_list.txt\", \"r\").read()\n",
    "STRING_LIST = SAMPLE_TEXTS.split(\"\\n\") * MULTIPLIER\n",
    "STRING_LIST = [sentence for sentence in STRING_LIST if len(sentence) > 0]\n",
    "# STRING_LIST = sorted(STRING_LIST, key=lambda s: len(s), reverse=True)\n",
    "\n",
    "sentence_bytes = [string_to_array(string, batch=False) for string in STRING_LIST]\n",
    "random.shuffle(sentence_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57ac56e-db66-4fd6-aff9-e8532e4548f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_decoder = ClientDecoder(TRITON_SERVER_URL, MODEL_NAME, MODEL_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5441a7f8-48a6-4360-a4f0-882a8bdb708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_decoder_async = ClientDecoderAsync(TRITON_SERVER_URL, MODEL_NAME, MODEL_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1248582e-5462-4965-a5eb-efd860daccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.843220710754395\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "responses = [\n",
    "    client_decoder.submit(sentence_byte, DEVICE) for sentence_byte in sentence_bytes\n",
    "]\n",
    "runtime = time.time() - start\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7829220b-57b4-4f30-97f0-627f013bc6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.779734373092651\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "async_responses = client_decoder_async.submit(sentence_bytes)\n",
    "runtime = time.time() - start\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cec576-e9e2-4d1b-9c1f-39f4ca3c8e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df763b7-2e89-46fb-9051-f2770d317e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
