{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from flair.data import Sentence\n",
    "from flair.models.sequence_tagger_utils.bioes import get_spans_from_bio\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea46d4-462c-470a-afb1-1bfadcc32c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_array(string, encoding=\"utf-8\"):\n",
    "    return np.asarray(list(bytes(string, encoding)))\n",
    "\n",
    "\n",
    "def bytes_to_string(byte_list):\n",
    "    return bytes(byte_list.tolist()).decode()\n",
    "\n",
    "\n",
    "class ClientDecoder:\n",
    "    def __init__(self, triton_server_url, model_name, model_version):\n",
    "        self.triton_client = grpcclient.InferenceServerClient(\n",
    "            url=triton_server_url, verbose=False\n",
    "        )\n",
    "\n",
    "        self.model_metadata = self.triton_client.get_model_metadata(\n",
    "            model_name=model_name, model_version=model_version\n",
    "        )\n",
    "\n",
    "        self.model_config = self.triton_client.get_model_config(\n",
    "            model_name=model_name, model_version=model_version\n",
    "        ).config\n",
    "        self.model_name = model_name\n",
    "        self.viterbi_decoder = torch.load(\n",
    "            \"/workspace/triton-models/flair-ner-english-fast-tokenization/1/viterbi_decoder.bin\"\n",
    "        )\n",
    "\n",
    "    def submit(self, sentence_bytes, device=\"cpu\"):\n",
    "        inputs = [\n",
    "            grpcclient.InferInput(\"sentence_bytes\", sentence_bytes.shape, \"INT64\"),\n",
    "        ]\n",
    "\n",
    "        inputs[0].set_data_from_numpy(sentence_bytes)\n",
    "\n",
    "        outputs = [grpcclient.InferRequestedOutput(\"tagged_sentences\")]\n",
    "\n",
    "        response = self.triton_client.infer(self.model_name, inputs, outputs=outputs)\n",
    "\n",
    "        tagged_sentences = torch.tensor(\n",
    "            response.as_numpy(\"tagged_sentences\"), device=DEVICE\n",
    "        )\n",
    "\n",
    "        return eval(bytes(tagged_sentences).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f076749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRITON_SERVER_URL = \"172.25.4.42:8001\"\n",
    "MODEL_NAME = \"flair-ner-english-fast-ensemble\"\n",
    "MODEL_VERSION = \"1\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "MULTIPLIER = 128\n",
    "SAMPLE_TEXTS = open(\"strings_list.txt\", \"r\").read()\n",
    "STRING_LIST = SAMPLE_TEXTS.split(\"\\n\") * MULTIPLIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ac56e-db66-4fd6-aff9-e8532e4548f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [string_to_array(string) for string in STRING_LIST]\n",
    "\n",
    "embeddings = torch.load(\n",
    "    \"/workspace/triton-models/flair-ner-english-fast-tokenization/1/embeddings.bin\",\n",
    "    map_location=torch.device(DEVICE),\n",
    ")\n",
    "\n",
    "viterbi_decoder = torch.load(\n",
    "    \"/workspace/triton-models/flair-ner-english-fast-tokenization/1/viterbi_decoder.bin\",\n",
    "    map_location=torch.device(DEVICE),\n",
    ")\n",
    "\n",
    "client_decoder = ClientDecoder(TRITON_SERVER_URL, MODEL_NAME, MODEL_VERSION)\n",
    "\n",
    "sentence_bytes = [string_to_array(string) for string in STRING_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500bb3fd-b7eb-4a90-be18-482d07129281",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bytes = [string_to_array(string) for string in STRING_LIST]\n",
    "sentence_bytes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248582e-5462-4965-a5eb-efd860daccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_total = len(sentence_bytes)\n",
    "pbar = tqdm(\n",
    "    total=est_total,\n",
    "    desc=\"Submitting sentences to {} at {}\".format(MODEL_NAME, TRITON_SERVER_URL),\n",
    ")\n",
    "\n",
    "responses = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for sentence_byte in sentence_bytes:\n",
    "        futures = []\n",
    "        futures.append(executor.submit(client_decoder.submit, sentence_byte, DEVICE))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            infer_results = future.result()\n",
    "            responses.append(infer_results)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829220b-57b4-4f30-97f0-627f013bc6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
