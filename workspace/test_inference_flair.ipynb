{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e6aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from flair.data import Sentence\n",
    "from flair.models.sequence_tagger_utils.bioes import get_spans_from_bio\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dea46d4-462c-470a-afb1-1bfadcc32c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_array(string, encoding=\"utf-8\"):\n",
    "    return np.asarray(list(bytes(string, encoding)))\n",
    "\n",
    "\n",
    "def bytes_to_string(byte_list):\n",
    "    return bytes(byte_list.tolist()).decode()\n",
    "\n",
    "\n",
    "class ClientDecoder:\n",
    "    def __init__(self, triton_server_url, model_name, model_version):\n",
    "        self.triton_client = grpcclient.InferenceServerClient(\n",
    "            url=triton_server_url, verbose=False\n",
    "        )\n",
    "\n",
    "        self.model_metadata = self.triton_client.get_model_metadata(\n",
    "            model_name=model_name, model_version=model_version\n",
    "        )\n",
    "\n",
    "        self.model_config = self.triton_client.get_model_config(\n",
    "            model_name=model_name, model_version=model_version\n",
    "        ).config\n",
    "        self.model_name = model_name\n",
    "        self.viterbi_decoder = torch.load(\n",
    "            \"/workspace/triton-models/flair-ner-english-fast-tokenization/1/viterbi_decoder.bin\"\n",
    "        )\n",
    "\n",
    "    def submit(self, sentence_bytes, device=\"cpu\"):\n",
    "        inputs = [\n",
    "            grpcclient.InferInput(\"sentence_bytes\", sentence_bytes.shape, \"INT64\"),\n",
    "        ]\n",
    "\n",
    "        inputs[0].set_data_from_numpy(sentence_bytes)\n",
    "\n",
    "        outputs = [grpcclient.InferRequestedOutput(\"tagged_sentences\")]\n",
    "\n",
    "        response = self.triton_client.infer(self.model_name, inputs, outputs=outputs)\n",
    "\n",
    "        tagged_sentences = torch.tensor(\n",
    "            response.as_numpy(\"tagged_sentences\"), device=DEVICE\n",
    "        )\n",
    "\n",
    "        return eval(bytes(tagged_sentences).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f076749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRITON_SERVER_URL = \"172.25.4.42:8001\"\n",
    "MODEL_NAME = \"flair-ner-english-fast-ensemble\"\n",
    "MODEL_VERSION = \"1\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "MULTIPLIER = 128\n",
    "SAMPLE_TEXTS = open(\"strings_list.txt\", \"r\").read()\n",
    "STRING_LIST = SAMPLE_TEXTS.split(\"\\n\") * MULTIPLIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57ac56e-db66-4fd6-aff9-e8532e4548f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "InferenceServerException",
     "evalue": "[StatusCode.UNAVAILABLE] failed to connect to all addresses",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/triton-models/flair-ner-english-fast-tokenization/1/embeddings.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(DEVICE),\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m viterbi_decoder \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/triton-models/flair-ner-english-fast-tokenization/1/viterbi_decoder.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(DEVICE),\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m client_decoder \u001b[38;5;241m=\u001b[39m \u001b[43mClientDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRITON_SERVER_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_VERSION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m sentence_bytes \u001b[38;5;241m=\u001b[39m [string_to_array(string) \u001b[38;5;28;01mfor\u001b[39;00m string \u001b[38;5;129;01min\u001b[39;00m STRING_LIST]\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mClientDecoder.__init__\u001b[0;34m(self, triton_server_url, model_name, model_version)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, triton_server_url, model_name, model_version):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriton_client \u001b[38;5;241m=\u001b[39m grpcclient\u001b[38;5;241m.\u001b[39mInferenceServerClient(\n\u001b[1;32m     12\u001b[0m         url\u001b[38;5;241m=\u001b[39mtriton_server_url, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriton_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriton_client\u001b[38;5;241m.\u001b[39mget_model_config(\n\u001b[1;32m     20\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mmodel_name, model_version\u001b[38;5;241m=\u001b[39mmodel_version\n\u001b[1;32m     21\u001b[0m     )\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tritonclient/grpc/__init__.py:514\u001b[0m, in \u001b[0;36mInferenceServerClient.get_model_metadata\u001b[0;34m(self, model_name, model_version, headers, as_json)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n\u001b[0;32m--> 514\u001b[0m     \u001b[43mraise_error_grpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrpc_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tritonclient/grpc/__init__.py:62\u001b[0m, in \u001b[0;36mraise_error_grpc\u001b[0;34m(rpc_error)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_error_grpc\u001b[39m(rpc_error):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m get_error_grpc(rpc_error) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: [StatusCode.UNAVAILABLE] failed to connect to all addresses"
     ]
    }
   ],
   "source": [
    "requests = [string_to_array(string) for string in STRING_LIST]\n",
    "\n",
    "embeddings = torch.load(\n",
    "    \"/workspace/triton-models/flair-ner-english-fast-tokenization/1/embeddings.bin\",\n",
    "    map_location=torch.device(DEVICE),\n",
    ")\n",
    "\n",
    "viterbi_decoder = torch.load(\n",
    "    \"/workspace/triton-models/flair-ner-english-fast-tokenization/1/viterbi_decoder.bin\",\n",
    "    map_location=torch.device(DEVICE),\n",
    ")\n",
    "\n",
    "client_decoder = ClientDecoder(TRITON_SERVER_URL, MODEL_NAME, MODEL_VERSION)\n",
    "\n",
    "sentence_bytes = [string_to_array(string) for string in STRING_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "500bb3fd-b7eb-4a90-be18-482d07129281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 78,  86,  73,  68,  73,  65,  32, 105, 115,  32, 102, 111, 117,\n",
       "       110, 100, 101, 100,  32,  98, 121,  32,  74, 101, 110, 115, 101,\n",
       "       110,  32,  72, 117,  97, 110, 103,  44,  32,  67, 104, 114, 105,\n",
       "       115,  32,  77,  97, 108,  97,  99, 104, 111, 119, 115, 107, 121,\n",
       "        32,  97, 110, 100,  32,  67, 117, 114, 116, 105, 115,  32,  80,\n",
       "       114, 105, 101, 109,  46])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bytes = [string_to_array(string) for string in STRING_LIST]\n",
    "sentence_bytes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248582e-5462-4965-a5eb-efd860daccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_total = len(sentence_bytes)\n",
    "pbar = tqdm(\n",
    "    total=est_total,\n",
    "    desc=\"Submitting sentences to {} at {}\".format(MODEL_NAME, TRITON_SERVER_URL),\n",
    ")\n",
    "\n",
    "responses = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for sentence_byte in sentence_bytes:\n",
    "        futures = []\n",
    "        futures.append(executor.submit(client_decoder.submit, sentence_byte, DEVICE))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            infer_results = future.result()\n",
    "            responses.append(infer_results)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829220b-57b4-4f30-97f0-627f013bc6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485d6a9-3c41-42a3-8794-610383964168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
